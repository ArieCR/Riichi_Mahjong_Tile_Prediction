{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:47:55.440510Z",
     "start_time": "2024-08-13T15:47:43.126951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from dataset_to_data import *\n",
    "dataset = MahjongDataset(data_path='database.db')\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from Data_Skimmer import *\n",
    "\n",
    "import time \n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv \n",
    "import torch.optim as optim\n",
    "start_time = time.time()\n",
    "X, y = [], []\n",
    "for i in range (1,2000):\n",
    "    temp = waits_data_proccessing(dataset[i])\n",
    "    if(temp == None):\n",
    "        continue\n",
    "    X.append(temp[0])\n",
    "    y.append(temp[1])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# zipping data into a database\n",
    "# Zip corresponding rows of X and y together\n",
    "zipped_data = [(X[i].tolist(), y[i].tolist()) for i in range(X.shape[0])]\n",
    "\n",
    "# Connect to SQLite database (or create it)\n",
    "conn = sqlite3.connect('test_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS test_table (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    X_values BLOB,\n",
    "    y_values BLOB\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert the zipped data into the table\n",
    "for i, (X_row, y_row) in enumerate(zipped_data):\n",
    "    cursor.execute('INSERT INTO test_table (X_values, y_values) VALUES (?, ?)', (str(X_row), str(y_row)))\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float)  # Use long for input data\n",
    "y = torch.tensor(y, dtype=torch.float)     # Use long for integer labels\n",
    "# until here!!!\n",
    "\n",
    "index1 = int(len(X)*0.8)\n",
    "index2 = int(len(X)*0.9)\n",
    "X_train = X[:index1]\n",
    "X_val = X[index1:index2]\n",
    "X_test = X[index2:]\n",
    "y_train = y[:index1]\n",
    "y_val = y[index1:index2]\n",
    "y_test = y[index2:]\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape, y.shape)  # Check shapes of tensors\n",
    "print(f\"Data processing time: {time.time() - start_time:.2f}s\")"
   ],
   "id": "7bbc6751a47a64b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([485, 368]) torch.Size([485, 34])\n",
      "Data processing time: 12.29s\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:49:48.071637Z",
     "start_time": "2024-08-13T15:49:46.605378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Block of extracting data from the database\n",
    "import ast\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('test_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query the data\n",
    "cursor.execute('SELECT X_values, y_values FROM test_table')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process the data\n",
    "X_extracted = []\n",
    "y_extracted = []\n",
    "\n",
    "for row in rows:\n",
    "    X_row = ast.literal_eval(row[0])  # Convert string back to list\n",
    "    y_row = ast.literal_eval(row[1])  # Convert string back to list\n",
    "    X_extracted.append(X_row)\n",
    "    y_extracted.append(y_row)\n",
    "\n",
    "# Convert lists back to NumPy arrays or tensors if needed\n",
    "import numpy as np\n",
    "\n",
    "X_extracted = np.array(X_extracted)\n",
    "y_extracted = np.array(y_extracted)\n",
    "\n",
    "# Optionally, convert to tensors\n",
    "X = torch.tensor(X_extracted, dtype=torch.float)\n",
    "y = torch.tensor(y_extracted, dtype=torch.float)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(X)\n",
    "print(y)"
   ],
   "id": "161e877252d31bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [2., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [2., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [2., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [2., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:56:19.982673Z",
     "start_time": "2024-08-13T14:53:48.386252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import  BertModel\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MahjongModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MahjongModel, self).__init__()\n",
    "        # Ensure the embedding dimension is compatible\n",
    "        #self.embedding = nn.Embedding(37, 128)\n",
    "        self.projection = nn.Linear(368, 768)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc2 = nn.Linear(768, 34)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        # Convert categorical features to embeddings\n",
    "        #x_emb = self.embedding(x_batch)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Project concatenated features to BERT's hidden size\n",
    "        #x = self.fc1(torch.cat((x_batch, x_emb)))  # Shape: (batch_size, hidden_size)\n",
    "        #x = self.fc1(x_batch)\n",
    "        # Add sequence length dimension for BERT\n",
    "        #x = x.unsqueeze(1)  # Shape: (batch_size, sequence_length=1, hidden_size)\n",
    "        x_batch = self.projection(x_batch)\n",
    "        \n",
    "        x_batch = x_batch.unsqueeze(1)\n",
    "        # Use BERT to process the combined features\n",
    "        outputs = self.bert(inputs_embeds=x_batch)[0]\n",
    "        \n",
    "        # Pass through classification head\n",
    "        outputs = self.fc2(outputs)\n",
    "        \n",
    "        y_hat = torch.sigmoid(outputs)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset \n",
    "class MahjongDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    \n",
    "def getindices(tensor):\n",
    "    mylist = []\n",
    "    for i in range(len(tensor)):\n",
    "        if tensor[i]:\n",
    "            mylist.append(i)\n",
    "    return mylist\n",
    "    \n",
    "    \n",
    "\n",
    "def eval(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        outputs = (outputs >= 0.5).float()\n",
    "        ran = random.randint(0, 1070)\n",
    "        print(getindices(outputs[ran][0]),getindices(y_val[ran]))\n",
    "        count = 0\n",
    "        for i in range(y_val.size(0)):\n",
    "            count += getindices(outputs[i][0]) == getindices(y_val[i])\n",
    "        total_samples = y_val.size(0)\n",
    "        misclassification_rate = 1.0 - (count / total_samples)\n",
    "        return misclassification_rate\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = \"cpu\"\n",
    "model = MahjongModel().to(device)\n",
    "\n",
    "train_dataset = MahjongDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = Adam(model.parameters() ,lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "  \n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    num_batches = 0\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "\n",
    "        \n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    print(\"miss is \", miss)\n",
    "    \n"
   ],
   "id": "d7b85b05253696c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/tmp/ipykernel_30508/2766732523.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_30508/2766732523.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.2907749398379521\n",
      "[] [3, 6]\n",
      "miss is  1.0\n",
      "Epoch  1  train loss is:  0.19707432749324572\n",
      "[] [22, 25]\n",
      "miss is  1.0\n",
      "Epoch  2  train loss is:  0.196144374883751\n",
      "[] [13, 16]\n",
      "miss is  1.0\n",
      "Epoch  3  train loss is:  0.19609373453381337\n",
      "[] [14, 17]\n",
      "miss is  1.0\n",
      "Epoch  4  train loss is:  0.1955728329801205\n",
      "[] [11]\n",
      "miss is  1.0\n",
      "Epoch  5  train loss is:  0.1953856410479457\n",
      "[] [10, 22]\n",
      "miss is  1.0\n",
      "Epoch  6  train loss is:  0.19523376575411475\n",
      "[] [12]\n",
      "miss is  1.0\n",
      "Epoch  7  train loss is:  0.19517767296404642\n",
      "[] [27, 30]\n",
      "miss is  1.0\n",
      "Epoch  8  train loss is:  0.19480729740126868\n",
      "[] [19]\n",
      "miss is  1.0\n",
      "Epoch  9  train loss is:  0.19486845974363803\n",
      "[] [29]\n",
      "miss is  1.0\n",
      "Epoch  10  train loss is:  0.19487275233056023\n",
      "[] [11, 14]\n",
      "miss is  1.0\n",
      "Epoch  11  train loss is:  0.194345602924939\n",
      "[] [11]\n",
      "miss is  1.0\n",
      "Epoch  12  train loss is:  0.1942223177523418\n",
      "[] [20]\n",
      "miss is  1.0\n",
      "Epoch  13  train loss is:  0.1939599982646318\n",
      "[] [15]\n",
      "miss is  1.0\n",
      "Epoch  14  train loss is:  0.1936817503773147\n",
      "[] [19, 22]\n",
      "miss is  1.0\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:40:56.655548Z",
     "start_time": "2024-08-13T15:19:50.236911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = Adam(model.parameters() ,lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(100): # can train for as long as you want\n",
    "    model.train()\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bert.encoder.layer[-5:].parameters():\n",
    "        param.requires_grad = True\n",
    "    num_batches = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    print(\"miss is \", miss)  "
   ],
   "id": "5d1d94de8fcb474c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30508/2766732523.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_30508/2766732523.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.08780361892011529\n",
      "[] [11]\n",
      "miss is  0.980410447761194\n",
      "Epoch  1  train loss is:  0.08638691429921243\n",
      "[8, 32] [4, 7]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  2  train loss is:  0.08452742246280816\n",
      "[20, 23] [0, 3]\n",
      "miss is  0.980410447761194\n",
      "Epoch  3  train loss is:  0.08363919953043576\n",
      "[21] [4, 25]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  4  train loss is:  0.08259138923318413\n",
      "[9] [9, 28]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  5  train loss is:  0.08155723425757043\n",
      "[2] [21, 24]\n",
      "miss is  0.9841417910447762\n",
      "Epoch  6  train loss is:  0.08078751575935729\n",
      "[1, 4] [14, 17]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  7  train loss is:  0.07975285753718539\n",
      "[] [22]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  8  train loss is:  0.07751960798987226\n",
      "[22, 25] [18, 21]\n",
      "miss is  0.9748134328358209\n",
      "Epoch  9  train loss is:  0.07657660102146266\n",
      "[] [22, 26]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  10  train loss is:  0.0767073558719628\n",
      "[] [14, 17]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  11  train loss is:  0.07482245693253319\n",
      "[15, 24] [18, 23]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  12  train loss is:  0.0742196702297957\n",
      "[] [11, 27]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  13  train loss is:  0.07280154294961004\n",
      "[21] [21]\n",
      "miss is  0.980410447761194\n",
      "Epoch  14  train loss is:  0.07182073742705207\n",
      "[] [11]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  15  train loss is:  0.07108355326003302\n",
      "[] [0, 3]\n",
      "miss is  0.980410447761194\n",
      "Epoch  16  train loss is:  0.07034437007315762\n",
      "[] [0, 33]\n",
      "miss is  0.976679104477612\n",
      "Epoch  17  train loss is:  0.06839037156182594\n",
      "[] [26, 31]\n",
      "miss is  0.980410447761194\n",
      "Epoch  18  train loss is:  0.06796884423184128\n",
      "[21] [1, 4]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  19  train loss is:  0.06767558901948113\n",
      "[] [24, 25]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  20  train loss is:  0.06687032477064647\n",
      "[1, 4] [3, 6]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  21  train loss is:  0.06616853638707927\n",
      "[] [0, 33]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  22  train loss is:  0.06468241039982073\n",
      "[] [20, 23]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  23  train loss is:  0.06401812407053092\n",
      "[] [11, 27]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  24  train loss is:  0.06281345877727168\n",
      "[] [19, 22]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  25  train loss is:  0.06215160004933084\n",
      "[11] [14, 32]\n",
      "miss is  0.9738805970149254\n",
      "Epoch  26  train loss is:  0.0618570412000205\n",
      "[1, 4] [20, 23]\n",
      "miss is  0.9710820895522388\n",
      "Epoch  27  train loss is:  0.05999356379462441\n",
      "[4] [26, 29]\n",
      "miss is  0.9701492537313433\n",
      "Epoch  28  train loss is:  0.060549681570007015\n",
      "[] [15]\n",
      "miss is  0.980410447761194\n",
      "Epoch  29  train loss is:  0.059900338986549236\n",
      "[12] [2]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  30  train loss is:  0.05792044100375867\n",
      "[] [11]\n",
      "miss is  0.980410447761194\n",
      "Epoch  31  train loss is:  0.05753147700406805\n",
      "[] [12, 25]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  32  train loss is:  0.05718603472616593\n",
      "[20] [5, 6]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  33  train loss is:  0.0561273069255968\n",
      "[28] [26]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  34  train loss is:  0.055783400138957794\n",
      "[] [11]\n",
      "miss is  0.976679104477612\n",
      "Epoch  35  train loss is:  0.055052691218910606\n",
      "[13, 31] [18, 21]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  36  train loss is:  0.054716664785346134\n",
      "[] [0, 33]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  37  train loss is:  0.05356708303315489\n",
      "[23] [14]\n",
      "miss is  0.976679104477612\n",
      "Epoch  38  train loss is:  0.05334843243697319\n",
      "[11] [2]\n",
      "miss is  0.9738805970149254\n",
      "Epoch  39  train loss is:  0.052568359113537244\n",
      "[11] [9, 12]\n",
      "miss is  0.9748134328358209\n",
      "Epoch  40  train loss is:  0.05130276517409374\n",
      "[1, 10, 13] [13, 16]\n",
      "miss is  0.976679104477612\n",
      "Epoch  41  train loss is:  0.05088171386419619\n",
      "[24] [11, 14]\n",
      "miss is  0.9738805970149254\n",
      "Epoch  42  train loss is:  0.05073322669419436\n",
      "[1] [2, 5, 8]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  43  train loss is:  0.04913564226468921\n",
      "[11, 14] [4, 21]\n",
      "miss is  0.9748134328358209\n",
      "Epoch  44  train loss is:  0.048978830975061456\n",
      "[28] [26]\n",
      "miss is  0.9720149253731343\n",
      "Epoch  45  train loss is:  0.04862016131651335\n",
      "[24] [0, 26]\n",
      "miss is  0.9748134328358209\n",
      "Epoch  46  train loss is:  0.04802587851328034\n",
      "[] [13, 16]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  47  train loss is:  0.04739566458922337\n",
      "[2] [14, 17]\n",
      "miss is  0.980410447761194\n",
      "Epoch  48  train loss is:  0.047028991771121006\n",
      "[5] [33]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  49  train loss is:  0.046149869065136274\n",
      "[] [6]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  50  train loss is:  0.045545355718042774\n",
      "[25] [0, 3]\n",
      "miss is  0.980410447761194\n",
      "Epoch  51  train loss is:  0.04599738681211126\n",
      "[20, 23] [0, 3]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  52  train loss is:  0.045335854130163514\n",
      "[] [18, 21]\n",
      "miss is  0.980410447761194\n",
      "Epoch  53  train loss is:  0.04441052216856453\n",
      "[] [11]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  54  train loss is:  0.04372067878292839\n",
      "[13] [12]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  55  train loss is:  0.04359139466041969\n",
      "[] [11]\n",
      "miss is  0.9729477611940298\n",
      "Epoch  56  train loss is:  0.04215535265307001\n",
      "[1, 31] [11]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  57  train loss is:  0.04229013956944739\n",
      "[] [20, 23]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  58  train loss is:  0.04202046680876978\n",
      "[2, 22, 25] [21, 24]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  59  train loss is:  0.040952911711259844\n",
      "[21, 24] [11, 14]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  60  train loss is:  0.04070086795213497\n",
      "[9, 12] [9, 28]\n",
      "miss is  0.9729477611940298\n",
      "Epoch  61  train loss is:  0.04063824559371489\n",
      "[6, 11] [3, 6]\n",
      "miss is  0.9710820895522388\n",
      "Epoch  62  train loss is:  0.0399358162940324\n",
      "[1] [6, 21]\n",
      "miss is  0.980410447761194\n",
      "Epoch  63  train loss is:  0.03947072958774504\n",
      "[] [11]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  64  train loss is:  0.03889414230811662\n",
      "[] [12]\n",
      "miss is  0.9757462686567164\n",
      "Epoch  65  train loss is:  0.038411343359426496\n",
      "[] [19, 22]\n",
      "miss is  0.980410447761194\n",
      "Epoch  66  train loss is:  0.03942443894353941\n",
      "[12] [13, 16]\n",
      "miss is  0.980410447761194\n",
      "Epoch  67  train loss is:  0.03805034868003711\n",
      "[] [2, 33]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  68  train loss is:  0.03776440587324059\n",
      "[23] [13]\n",
      "miss is  0.980410447761194\n",
      "Epoch  69  train loss is:  0.0363772385667137\n",
      "[28] [26]\n",
      "miss is  0.9860074626865671\n",
      "Epoch  70  train loss is:  0.03734491190779608\n",
      "[1, 31] [11]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  71  train loss is:  0.03580574565272792\n",
      "[3] [10]\n",
      "miss is  0.9869402985074627\n",
      "Epoch  72  train loss is:  0.03544586894819063\n",
      "[1, 4] [20, 23]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  73  train loss is:  0.03575312844079445\n",
      "[11] [14, 17]\n",
      "miss is  0.9869402985074627\n",
      "Epoch  74  train loss is:  0.035074318554875576\n",
      "[4] [19, 22]\n",
      "miss is  0.976679104477612\n",
      "Epoch  75  train loss is:  0.03497891454891645\n",
      "[20] [3, 6]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  76  train loss is:  0.03456531576881511\n",
      "[4] [10, 11, 13]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  77  train loss is:  0.03454965149090525\n",
      "[] [11]\n",
      "miss is  0.9776119402985075\n",
      "Epoch  78  train loss is:  0.03418816539171571\n",
      "[29] [29]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  79  train loss is:  0.03368838484568667\n",
      "[1, 4] [3, 6]\n",
      "miss is  0.980410447761194\n",
      "Epoch  80  train loss is:  0.03297296252627909\n",
      "[] [18, 21]\n",
      "miss is  0.9794776119402985\n",
      "Epoch  81  train loss is:  0.03298911589395158\n",
      "[1, 31] [11]\n",
      "miss is  0.9785447761194029\n",
      "Epoch  82  train loss is:  0.03328338980702441\n",
      "[] [1, 4]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  83  train loss is:  0.032107819453380364\n",
      "[] [22, 26]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  84  train loss is:  0.03221718864827019\n",
      "[3, 4] [22, 25]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  85  train loss is:  0.03155770158346701\n",
      "[21] [24, 25]\n",
      "miss is  0.9869402985074627\n",
      "Epoch  86  train loss is:  0.031358662535709965\n",
      "[] [6]\n",
      "miss is  0.9841417910447762\n",
      "Epoch  87  train loss is:  0.03101988044328517\n",
      "[6] [14, 17]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  88  train loss is:  0.03131408244371414\n",
      "[20, 23] [11, 14]\n",
      "miss is  0.9813432835820896\n",
      "Epoch  89  train loss is:  0.030667284739006408\n",
      "[] [11, 14]\n",
      "miss is  0.9822761194029851\n",
      "Epoch  90  train loss is:  0.03041320898982118\n",
      "[11] [1, 4, 7]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  91  train loss is:  0.029714024656878085\n",
      "[1, 4] [15]\n",
      "miss is  0.9832089552238806\n",
      "Epoch  92  train loss is:  0.030314909879353633\n",
      "[1, 4] [3, 6]\n",
      "miss is  0.9841417910447762\n",
      "Epoch  93  train loss is:  0.029306856240232416\n",
      "[12] [13, 16]\n",
      "miss is  0.9841417910447762\n",
      "Epoch  94  train loss is:  0.029541410671955592\n",
      "[27, 28] [4, 24]\n",
      "miss is  0.980410447761194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(y_pred_tf, labels)\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m num_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    520\u001B[0m     )\n\u001B[0;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
