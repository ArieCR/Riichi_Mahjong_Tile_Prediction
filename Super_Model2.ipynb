{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.154472Z",
     "start_time": "2024-08-14T20:48:05.513976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import sqlite3\n",
    "import torch\n",
    "import time\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('tenhou_dataprocess/dst/2016-2020_after_script_waits.db')\n",
    "cursor = conn.cursor()\n",
    "# Query the data\n",
    "start_time = time.time()\n",
    "cursor.execute('SELECT X_values, y_values FROM test_table')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process the data\n",
    "X_extracted = []\n",
    "y_extracted = []\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    counter += 1\n",
    "    X_row = ast.literal_eval(row[0])  # Convert string back to list\n",
    "    y_row = ast.literal_eval(row[1])  # Convert string back to list\n",
    "    X_extracted.append(X_row)\n",
    "    y_extracted.append(y_row)\n",
    "    if(counter//1000 == 500):\n",
    "        break\n",
    "    if(counter % 1000 == 0):\n",
    "        print(f\"Processed {counter} rows\", end='\\r')\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "# Convert lists back to NumPy arrays or tensors if needed\n",
    "import numpy as np\n",
    "\n",
    "X_extracted = np.array(X_extracted)\n",
    "y_extracted = np.array(y_extracted)\n",
    "\n",
    "# Optionally, convert to tensors\n",
    "X = torch.tensor(X_extracted, dtype=torch.float)\n",
    "y = torch.tensor(y_extracted, dtype=torch.float)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(X.shape, y.shape)  # Check shapes of tensors"
   ],
   "id": "161e877252d31bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500000, 368]) torch.Size([500000, 34])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.159537Z",
     "start_time": "2024-08-14T20:57:43.155921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split the data into train, validation and test\n",
    "index1 = int(len(X)*0.8)\n",
    "index2 = int(len(X)*0.9)\n",
    "X_train = X[:index1]\n",
    "X_val = X[index1:index2]\n",
    "X_test = X[index2:]\n",
    "y_train = y[:index1]\n",
    "y_val = y[index1:index2]\n",
    "y_test = y[index2:]"
   ],
   "id": "f8f682db447d6a64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T21:38:08.216167Z",
     "start_time": "2024-08-14T20:57:43.160877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import  BertModel\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MahjongModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MahjongModel, self).__init__()\n",
    "        #self.embedding = nn.Embedding(37, 128)\n",
    "        #self.projection = nn.Linear(368, 400)\n",
    "        self.projection = nn.Linear(368, 768)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc2 = nn.Linear(768, 34)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        # Convert categorical features to embeddings\n",
    "        #x_emb = self.embedding(x_batch)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Project concatenated features to BERT's hidden size\n",
    "        #x = self.fc1(torch.cat((x_batch, x_emb)))  # Shape: (batch_size, hidden_size)\n",
    "        #x = self.fc1(x_batch)\n",
    "        # Add sequence length dimension for BERT\n",
    "        #x = x.unsqueeze(1)  # Shape: (batch_size, sequence_length=1, hidden_size)\n",
    "        x_batch = self.projection(x_batch)\n",
    "        \n",
    "        x_batch = x_batch.unsqueeze(1)\n",
    "        # Use BERT to process the combined features\n",
    "        outputs = self.bert(inputs_embeds=x_batch)[0]\n",
    "        \n",
    "        # Pass through classification head\n",
    "        outputs = self.fc2(outputs)\n",
    "        \n",
    "        y_hat = torch.sigmoid(outputs)\n",
    "        return y_hat\n",
    "    \n",
    "from torch.utils.data import Dataset \n",
    "class MahjongDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    \n",
    "def getindices(tensor):\n",
    "    mylist = []\n",
    "    for i in range(len(tensor)):\n",
    "        if tensor[i]:\n",
    "            mylist.append(i)\n",
    "    return mylist\n",
    "    \n",
    "    \n",
    "\n",
    "def eval(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        outputs = (outputs >= 0.5).float()\n",
    "        ran = random.randint(0, 1070)\n",
    "        print(getindices(outputs[ran][0]),getindices(y_val[ran]))\n",
    "        count = 0\n",
    "        for i in range(y_val.size(0)):\n",
    "            count += getindices(outputs[i][0]) == getindices(y_val[i])\n",
    "        total_samples = y_val.size(0)\n",
    "        misclassification_rate = 1.0 - (count / total_samples)\n",
    "        return misclassification_rate\n",
    "\n",
    "# Example Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MahjongModel().to(device)\n",
    "\n",
    "train_dataset = MahjongDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = Adam(model.parameters() ,lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    num_batches = 0\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "\n",
    "        \n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    print(\"miss is \", miss)\n",
    "    \n",
    "g"
   ],
   "id": "d7b85b05253696c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/tmp/ipykernel_5162/3103256056.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3103256056.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.20009591043949126\n",
      "[] [12, 15]\n",
      "miss is  1.0\n",
      "Epoch  1  train loss is:  0.19722917486548425\n",
      "[] [23, 26]\n",
      "miss is  1.0\n",
      "Epoch  2  train loss is:  0.1966469174873829\n",
      "[] [11, 14]\n",
      "miss is  1.0\n",
      "Epoch  3  train loss is:  0.19628167842030525\n",
      "[] [5]\n",
      "miss is  1.0\n",
      "Epoch  4  train loss is:  0.19600479583859443\n",
      "[] [5, 14]\n",
      "miss is  1.0\n",
      "Epoch  5  train loss is:  0.19578451664447785\n",
      "[] [4, 7]\n",
      "miss is  1.0\n",
      "Epoch  6  train loss is:  0.19562475204229354\n",
      "[] [22]\n",
      "miss is  1.0\n",
      "Epoch  7  train loss is:  0.19549327293515206\n",
      "[] [14, 24]\n",
      "miss is  1.0\n",
      "Epoch  8  train loss is:  0.19535006182074546\n",
      "[] [14]\n",
      "miss is  1.0\n",
      "Epoch  9  train loss is:  0.19520667307138442\n",
      "[] [10, 27]\n",
      "miss is  1.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T21:38:08.222358Z",
     "start_time": "2024-08-14T21:38:08.220165Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bcef878cb297e6a6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-14T21:38:08.223615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Real training\n",
    "optimizer = Adam(model.parameters() ,lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(100): # can train for as long as you want\n",
    "    model.train()\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bert.encoder.layer[-5:].parameters():\n",
    "        param.requires_grad = True\n",
    "    num_batches = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    print(\"miss is \", miss)  \n"
   ],
   "id": "5d1d94de8fcb474c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5162/3103256056.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3103256056.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.1945283597254753\n",
      "[] [18, 21]\n",
      "miss is  1.0\n",
      "Epoch  1  train loss is:  0.1941623804628849\n",
      "[] [14]\n",
      "miss is  1.0\n",
      "Epoch  2  train loss is:  0.19386281734824182\n",
      "[] [2, 5]\n",
      "miss is  1.0\n",
      "Epoch  3  train loss is:  0.19359265900492667\n",
      "[] [9, 12]\n",
      "miss is  1.0\n",
      "Epoch  4  train loss is:  0.19320041516661643\n",
      "[] [22, 25]\n",
      "miss is  1.0\n",
      "Epoch  5  train loss is:  0.192711581376791\n",
      "[] [10, 13]\n",
      "miss is  1.0\n",
      "Epoch  6  train loss is:  0.19198876527905465\n",
      "[] [22, 25]\n",
      "miss is  1.0\n",
      "Epoch  7  train loss is:  0.1910808652806282\n",
      "[] [10, 13]\n",
      "miss is  1.0\n",
      "Epoch  8  train loss is:  0.1898768885612488\n",
      "[] [0]\n",
      "miss is  1.0\n",
      "Epoch  9  train loss is:  0.18836382184267045\n",
      "[] [4, 7]\n",
      "miss is  0.99998\n",
      "Epoch  10  train loss is:  0.18654899177908899\n",
      "[] [23, 25]\n",
      "miss is  0.99996\n",
      "Epoch  11  train loss is:  0.18435725679397583\n",
      "[] [19]\n",
      "miss is  0.99986\n",
      "Epoch  12  train loss is:  0.18182365193128586\n",
      "[] [23, 25]\n",
      "miss is  0.9998\n",
      "Epoch  13  train loss is:  0.1788601653790474\n",
      "[] [15, 33]\n",
      "miss is  0.9997\n",
      "Epoch  14  train loss is:  0.17574133799552918\n",
      "[] [4, 20, 23]\n",
      "miss is  0.9992\n",
      "Epoch  15  train loss is:  0.17233935507774353\n",
      "[] [14]\n",
      "miss is  0.99918\n",
      "Epoch  16  train loss is:  0.16859516857743262\n",
      "[] [10]\n",
      "miss is  0.99862\n",
      "Epoch  17  train loss is:  0.1648051772803068\n",
      "[] [10, 13]\n",
      "miss is  0.99838\n",
      "Epoch  18  train loss is:  0.16083798815906047\n",
      "[] [22]\n",
      "miss is  0.99772\n",
      "Epoch  19  train loss is:  0.1567460195606947\n",
      "[] [24]\n",
      "miss is  0.99816\n",
      "Epoch  20  train loss is:  0.15265609190285206\n",
      "[] [9, 12]\n",
      "miss is  0.99766\n",
      "Epoch  21  train loss is:  0.1484533525335789\n",
      "[] [19, 22]\n",
      "miss is  0.99712\n",
      "Epoch  22  train loss is:  0.1443624816286564\n",
      "[22] [11, 14]\n",
      "miss is  0.99688\n",
      "Epoch  23  train loss is:  0.14030579448282718\n",
      "[22] [13, 16]\n",
      "miss is  0.99678\n",
      "Epoch  24  train loss is:  0.1362988475525379\n",
      "[] [19, 22]\n",
      "miss is  0.99708\n",
      "Epoch  25  train loss is:  0.1321762517338991\n",
      "[1, 4] [23, 26]\n",
      "miss is  0.99698\n",
      "Epoch  26  train loss is:  0.12842822232246398\n",
      "[20, 23] [0, 3]\n",
      "miss is  0.99696\n",
      "Epoch  27  train loss is:  0.12459073660254479\n",
      "[] [19, 22]\n",
      "miss is  0.99678\n",
      "Epoch  28  train loss is:  0.12078220187604427\n",
      "[12, 15] [14]\n",
      "miss is  0.99638\n",
      "Epoch  29  train loss is:  0.11701206397414207\n",
      "[] [22, 25]\n",
      "miss is  0.99662\n",
      "Epoch  30  train loss is:  0.11354638283967972\n",
      "[26] [21]\n",
      "miss is  0.99558\n",
      "Epoch  31  train loss is:  0.11012310793757439\n",
      "[] [3, 22]\n",
      "miss is  0.99578\n",
      "Epoch  32  train loss is:  0.10679285399734974\n",
      "[20, 23] [19, 22]\n",
      "miss is  0.99536\n",
      "Epoch  33  train loss is:  0.10358858109503985\n",
      "[] [0, 26]\n",
      "miss is  0.9954\n",
      "Epoch  34  train loss is:  0.10042138724595308\n",
      "[12, 15] [14]\n",
      "miss is  0.9953\n",
      "Epoch  35  train loss is:  0.0973776097303629\n",
      "[] [11, 14]\n",
      "miss is  0.99532\n",
      "Epoch  36  train loss is:  0.09443969762384892\n",
      "[] [24]\n",
      "miss is  0.99494\n",
      "Epoch  37  train loss is:  0.09175349531173706\n",
      "[24] [7, 11]\n",
      "miss is  0.99452\n",
      "Epoch  38  train loss is:  0.08905108315825462\n",
      "[23] [21]\n",
      "miss is  0.9948\n",
      "Epoch  39  train loss is:  0.08643186537891627\n",
      "[] [2, 5]\n",
      "miss is  0.99424\n",
      "Epoch  40  train loss is:  0.08407696570247412\n",
      "[2] [4, 20, 23]\n",
      "miss is  0.99422\n",
      "Epoch  41  train loss is:  0.08154397139310837\n",
      "[10] [21]\n",
      "miss is  0.99394\n",
      "Epoch  42  train loss is:  0.07918655812770128\n",
      "[] [22, 25]\n",
      "miss is  0.99466\n",
      "Epoch  43  train loss is:  0.07708407157510519\n",
      "[18] [11, 14]\n",
      "miss is  0.99334\n",
      "Epoch  44  train loss is:  0.07503508747383952\n",
      "[] [25]\n",
      "miss is  0.99434\n",
      "Epoch  45  train loss is:  0.07297628184691071\n",
      "[22] [10]\n",
      "miss is  0.9942\n",
      "Epoch  46  train loss is:  0.07101228988170624\n",
      "[0, 3] [19, 22]\n",
      "miss is  0.99348\n",
      "Epoch  47  train loss is:  0.06910554406046868\n",
      "[27] [8, 32]\n",
      "miss is  0.99354\n",
      "Epoch  48  train loss is:  0.06728735052064061\n",
      "[] [3, 6]\n",
      "miss is  0.99364\n",
      "Epoch  49  train loss is:  0.06541422164961695\n",
      "[] [10]\n",
      "miss is  0.99434\n",
      "Epoch  50  train loss is:  0.06372071180254221\n",
      "[18] [23, 26]\n",
      "miss is  0.9938\n",
      "Epoch  51  train loss is:  0.06217392793148756\n",
      "[15] [20]\n",
      "miss is  0.99406\n",
      "Epoch  52  train loss is:  0.06057699739649892\n",
      "[25] [14, 32]\n",
      "miss is  0.99408\n",
      "Epoch  53  train loss is:  0.059125724666565654\n",
      "[25] [14, 32]\n",
      "miss is  0.9938\n",
      "Epoch  54  train loss is:  0.05770919856995344\n",
      "[18, 30] [11, 14]\n",
      "miss is  0.99296\n",
      "Epoch  55  train loss is:  0.05630056163057685\n",
      "[] [10, 13]\n",
      "miss is  0.99418\n",
      "Epoch  56  train loss is:  0.05497908994674683\n",
      "[3] [2, 5]\n",
      "miss is  0.993\n",
      "Epoch  57  train loss is:  0.05373077801436186\n",
      "[17] [20, 23]\n",
      "miss is  0.99374\n",
      "Epoch  58  train loss is:  0.05241297175005078\n",
      "[] [2, 5]\n",
      "miss is  0.99326\n",
      "Epoch  59  train loss is:  0.051185995648950336\n",
      "[23, 28] [19, 22]\n",
      "miss is  0.99382\n",
      "Epoch  60  train loss is:  0.04985015914067626\n",
      "[] [21]\n",
      "miss is  0.99378\n",
      "Epoch  61  train loss is:  0.04890267488710582\n",
      "[11] [3]\n",
      "miss is  0.99394\n",
      "Epoch  62  train loss is:  0.047758356229290366\n",
      "[] [1]\n",
      "miss is  0.99334\n",
      "Epoch  63  train loss is:  0.046588186110779646\n",
      "[4] [3, 6]\n",
      "miss is  0.99372\n",
      "Epoch  64  train loss is:  0.04555596829988062\n",
      "[14] [4, 32]\n",
      "miss is  0.99372\n",
      "Epoch  65  train loss is:  0.0445709551423043\n",
      "[] [20, 23]\n",
      "miss is  0.99366\n",
      "Epoch  66  train loss is:  0.043660762920081614\n",
      "[2] [22]\n",
      "miss is  0.9938\n",
      "Epoch  67  train loss is:  0.042647317141592506\n",
      "[13, 16] [13, 26]\n",
      "miss is  0.99306\n",
      "Epoch  68  train loss is:  0.041712353321164845\n",
      "[0] [12, 15]\n",
      "miss is  0.99264\n",
      "Epoch  69  train loss is:  0.0408359201092273\n",
      "[] [3, 6]\n",
      "miss is  0.993\n",
      "Epoch  70  train loss is:  0.03995880556203425\n",
      "[15] [4, 20, 23]\n",
      "miss is  0.99354\n",
      "Epoch  71  train loss is:  0.03914228788156062\n",
      "[] [1]\n",
      "miss is  0.99272\n",
      "Epoch  72  train loss is:  0.03833917240589857\n",
      "[0, 3] [19, 22]\n",
      "miss is  0.9925\n",
      "Epoch  73  train loss is:  0.03749996238529682\n",
      "[] [7, 11]\n",
      "miss is  0.99318\n",
      "Epoch  74  train loss is:  0.03665515814654529\n",
      "[] [2, 5, 8]\n",
      "miss is  0.99308\n",
      "Epoch  75  train loss is:  0.03601096023403108\n",
      "[] [1]\n",
      "miss is  0.9935\n",
      "Epoch  76  train loss is:  0.03514807197328657\n",
      "[] [15, 32]\n",
      "miss is  0.99374\n",
      "Epoch  77  train loss is:  0.03444379608530551\n",
      "[] [22, 25]\n",
      "miss is  0.9937\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "model._save_to_state_dict('model.pth')",
   "id": "ee5556e7b80b08c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('model.pth'))",
   "id": "f197bdd40d7e252d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
