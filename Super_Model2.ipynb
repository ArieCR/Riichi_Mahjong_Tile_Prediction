{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.154472Z",
     "start_time": "2024-08-14T20:48:05.513976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import sqlite3\n",
    "import torch\n",
    "import time\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('tenhou_dataprocess/dst/2016-2020_after_script_waits.db')\n",
    "cursor = conn.cursor()\n",
    "# Query the data\n",
    "start_time = time.time()\n",
    "cursor.execute('SELECT X_values, y_values FROM test_table')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process the data\n",
    "X_extracted = []\n",
    "y_extracted = []\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    counter += 1\n",
    "    X_row = ast.literal_eval(row[0])  # Convert string back to list\n",
    "    y_row = ast.literal_eval(row[1])  # Convert string back to list\n",
    "    X_extracted.append(X_row)\n",
    "    y_extracted.append(y_row)\n",
    "    if(counter//1000 == 500):\n",
    "        break\n",
    "    if(counter % 1000 == 0):\n",
    "        print(f\"Processed {counter} rows\", end='\\r')\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "# Convert lists back to NumPy arrays or tensors if needed\n",
    "import numpy as np\n",
    "\n",
    "X_extracted = np.array(X_extracted)\n",
    "y_extracted = np.array(y_extracted)\n",
    "\n",
    "# Optionally, convert to tensors\n",
    "X = torch.tensor(X_extracted, dtype=torch.float)\n",
    "y = torch.tensor(y_extracted, dtype=torch.float)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(X.shape, y.shape)  # Check shapes of tensors"
   ],
   "id": "161e877252d31bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500000, 368]) torch.Size([500000, 34])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.159537Z",
     "start_time": "2024-08-14T20:57:43.155921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split the data into train, validation and test\n",
    "index1 = int(len(X)*0.8)\n",
    "index2 = int(len(X)*0.9)\n",
    "X_train = X[:index1]\n",
    "X_val = X[index1:index2]\n",
    "X_test = X[index2:]\n",
    "y_train = y[:index1]\n",
    "y_val = y[index1:index2]\n",
    "y_test = y[index2:]"
   ],
   "id": "f8f682db447d6a64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T10:20:12.582222Z",
     "start_time": "2024-08-15T09:57:04.601912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import  BertModel\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MahjongModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MahjongModel, self).__init__()\n",
    "        #self.embedding = nn.Embedding(37, 128)\n",
    "        #self.projection = nn.Linear(368, 400)\n",
    "        self.projection = nn.Linear(368, 768)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc2 = nn.Linear(768, 34)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        # Convert categorical features to embeddings\n",
    "        #x_emb = self.embedding(x_batch)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Project concatenated features to BERT's hidden size\n",
    "        #x = self.fc1(torch.cat((x_batch, x_emb)))  # Shape: (batch_size, hidden_size)\n",
    "        #x = self.fc1(x_batch)\n",
    "        # Add sequence length dimension for BERT\n",
    "        #x = x.unsqueeze(1)  # Shape: (batch_size, sequence_length=1, hidden_size)\n",
    "        x_batch = self.projection(x_batch)\n",
    "        \n",
    "        x_batch = x_batch.unsqueeze(1)\n",
    "        # Use BERT to process the combined features\n",
    "        outputs = self.bert(inputs_embeds=x_batch)[0]\n",
    "        \n",
    "        # Pass through classification head\n",
    "        outputs = self.fc2(outputs)\n",
    "        \n",
    "        y_hat = torch.sigmoid(outputs)\n",
    "        return y_hat\n",
    "    \n",
    "from torch.utils.data import Dataset \n",
    "class MahjongDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    \n",
    "def getindices(tensor):\n",
    "    mylist = []\n",
    "    for i in range(len(tensor)):\n",
    "        if tensor[i]:\n",
    "            mylist.append(i)\n",
    "    return mylist\n",
    "    \n",
    "    \n",
    "\n",
    "def eval(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        outputs = (outputs >= 0.5).float()\n",
    "        ran = random.randint(0, 1070)\n",
    "        print(getindices(outputs[ran][0]),getindices(y_val[ran]))\n",
    "        count = 0\n",
    "        for i in range(y_val.size(0)):\n",
    "            count += getindices(outputs[i][0]) == getindices(y_val[i])\n",
    "        total_samples = y_val.size(0)\n",
    "        misclassification_rate = 1.0 - (count / total_samples)\n",
    "        return misclassification_rate\n",
    "\n",
    "# Example Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MahjongModel().to(device)\n",
    "\n",
    "\n",
    "train_dataset = MahjongDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = Adam(model.parameters() ,lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    num_batches = 0\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "\n",
    "        \n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    print(\"Time taken for epoch \", epoch, \" is \", time.time()-start)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    start = time.time()\n",
    "    print(\"miss is \", miss, \"Time taken for eval is \", time.time()-start)\n",
    "    "
   ],
   "id": "d7b85b05253696c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/tmp/ipykernel_5162/3654550642.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3654550642.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.2259479618400259\n",
      "[] [3, 22]\n",
      "miss is  1.0\n",
      "Epoch  1  train loss is:  0.1978073485405244\n",
      "[] [19, 22]\n",
      "miss is  1.0\n",
      "Epoch  2  train loss is:  0.19754531432676803\n",
      "[] [14]\n",
      "miss is  1.0\n",
      "Epoch  3  train loss is:  0.19738119791078446\n",
      "[] [6, 28]\n",
      "miss is  1.0\n",
      "Epoch  4  train loss is:  0.19725125044812936\n",
      "[] [10]\n",
      "miss is  1.0\n",
      "Epoch  5  train loss is:  0.1971239298391525\n",
      "[] [18, 21, 24]\n",
      "miss is  1.0\n",
      "Epoch  6  train loss is:  0.19699102344796482\n",
      "[] [10, 27]\n",
      "miss is  1.0\n",
      "Epoch  7  train loss is:  0.196775687701257\n",
      "[] [19, 22]\n",
      "miss is  1.0\n",
      "Epoch  8  train loss is:  0.19650366382144602\n",
      "[] [2, 5, 8]\n",
      "miss is  1.0\n",
      "Epoch  9  train loss is:  0.19627205997019473\n",
      "[] [2, 5]\n",
      "miss is  1.0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T21:38:08.222358Z",
     "start_time": "2024-08-14T21:38:08.220165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = MahjongDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2048, shuffle=True)"
   ],
   "id": "bcef878cb297e6a6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-15T10:59:31.141035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Real training\n",
    "optimizer = Adam(model.parameters() ,lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(100): # can train for as long as you want\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bert.encoder.layer[-6:].parameters():\n",
    "        param.requires_grad = True\n",
    "    num_batches = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches, \"Time taken for epoch \", epoch, \" is \", time.time()-start)\n",
    "    start = time.time()\n",
    "    if(epoch % 5 == 0):    \n",
    "        miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "        print(\"Miss is \", miss, \"Time taken for eval is \", time.time()-start)  \n"
   ],
   "id": "5d1d94de8fcb474c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5162/3654550642.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3654550642.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.15841774023173716 Time taken for epoch  0  is  112.30009579658508\n",
      "[] [10, 13]\n",
      "Miss is  0.99744 Time taken for eval is  45.014870166778564\n",
      "Epoch  1  train loss is:  0.15118931527332882 Time taken for epoch  1  is  113.00276589393616\n",
      "Epoch  2  train loss is:  0.14400003837121417 Time taken for epoch  2  is  122.47662568092346\n",
      "Epoch  3  train loss is:  0.1369739359106554 Time taken for epoch  3  is  126.85388088226318\n",
      "Epoch  4  train loss is:  0.13022839528558505 Time taken for epoch  4  is  174.46911644935608\n",
      "Epoch  5  train loss is:  0.12355312764111077 Time taken for epoch  5  is  133.2139208316803\n",
      "[] [2, 4, 5, 7, 8]\n",
      "Miss is  0.9954 Time taken for eval is  50.22937297821045\n",
      "Epoch  6  train loss is:  0.11751224530284363 Time taken for epoch  6  is  129.43603014945984\n",
      "Epoch  7  train loss is:  0.11175660018230338 Time taken for epoch  7  is  117.39853882789612\n",
      "Epoch  8  train loss is:  0.10630872547435943 Time taken for epoch  8  is  107.04766798019409\n",
      "Epoch  9  train loss is:  0.10129817877241108 Time taken for epoch  9  is  106.91137647628784\n",
      "Epoch  10  train loss is:  0.09666341426008193 Time taken for epoch  10  is  108.12636375427246\n",
      "[] [24]\n",
      "Miss is  0.99388 Time taken for eval is  46.00243854522705\n",
      "Epoch  11  train loss is:  0.09231475435788064 Time taken for epoch  11  is  109.83790302276611\n",
      "Epoch  12  train loss is:  0.08823131917573301 Time taken for epoch  12  is  109.24590730667114\n",
      "Epoch  13  train loss is:  0.08444585572080233 Time taken for epoch  13  is  107.0585207939148\n",
      "Epoch  14  train loss is:  0.08094761049960886 Time taken for epoch  14  is  109.5847225189209\n",
      "Epoch  15  train loss is:  0.07764037538443685 Time taken for epoch  15  is  122.31017136573792\n",
      "[] [20]\n",
      "Miss is  0.9928 Time taken for eval is  48.06082820892334\n",
      "Epoch  16  train loss is:  0.07455863662616675 Time taken for epoch  16  is  126.69789600372314\n",
      "Epoch  17  train loss is:  0.0716316303252564 Time taken for epoch  17  is  120.16257047653198\n",
      "Epoch  18  train loss is:  0.06885949877636208 Time taken for epoch  18  is  107.87677192687988\n",
      "Epoch  19  train loss is:  0.06641772871508318 Time taken for epoch  19  is  106.69193601608276\n",
      "Epoch  20  train loss is:  0.0639176548784956 Time taken for epoch  20  is  107.34398937225342\n",
      "[] [20]\n",
      "Miss is  0.99248 Time taken for eval is  41.46509909629822\n",
      "Epoch  21  train loss is:  0.06169189148299072 Time taken for epoch  21  is  108.52031636238098\n",
      "Epoch  22  train loss is:  0.05947076010486811 Time taken for epoch  22  is  107.8983690738678\n",
      "Epoch  23  train loss is:  0.05752872609916855 Time taken for epoch  23  is  106.39965462684631\n",
      "Epoch  24  train loss is:  0.05540202256492184 Time taken for epoch  24  is  107.38911652565002\n",
      "Epoch  25  train loss is:  0.05357484069779096 Time taken for epoch  25  is  107.16659379005432\n",
      "[2] [19, 22]\n",
      "Miss is  0.99106 Time taken for eval is  39.78839564323425\n",
      "Epoch  26  train loss is:  0.05173320609533116 Time taken for epoch  26  is  106.19622445106506\n",
      "Epoch  27  train loss is:  0.04997666685572823 Time taken for epoch  27  is  107.18704319000244\n",
      "Epoch  28  train loss is:  0.04852246354950968 Time taken for epoch  28  is  112.71840500831604\n",
      "Epoch  29  train loss is:  0.04688182547021552 Time taken for epoch  29  is  110.1745867729187\n",
      "Epoch  30  train loss is:  0.04530165281118182 Time taken for epoch  30  is  105.26725912094116\n",
      "[12] [19]\n",
      "Miss is  0.99192 Time taken for eval is  39.336450815200806\n",
      "Epoch  31  train loss is:  0.043859444325194334 Time taken for epoch  31  is  104.9486837387085\n",
      "Epoch  32  train loss is:  0.04249236268727371 Time taken for epoch  32  is  105.56769061088562\n",
      "Epoch  33  train loss is:  0.04116233499706401 Time taken for epoch  33  is  105.69073605537415\n",
      "Epoch  34  train loss is:  0.03996665505906734 Time taken for epoch  34  is  105.32707643508911\n",
      "Epoch  35  train loss is:  0.038679252735927434 Time taken for epoch  35  is  111.5973870754242\n",
      "[19, 22] [11, 22]\n",
      "Miss is  0.99094 Time taken for eval is  43.085553884506226\n",
      "Epoch  36  train loss is:  0.037371927729386197 Time taken for epoch  36  is  113.72902703285217\n",
      "Epoch  37  train loss is:  0.036427845930694924 Time taken for epoch  37  is  107.41913604736328\n",
      "Epoch  38  train loss is:  0.03524317655264569 Time taken for epoch  38  is  108.31748485565186\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:12:40.667060Z",
     "start_time": "2024-08-15T08:12:40.650150Z"
    }
   },
   "cell_type": "code",
   "source": "model._save_to_state_dict('model.pth',keep_vars=True,prefix='model')",
   "id": "ee5556e7b80b08c0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module._save_to_state_dict() missing 2 required positional arguments: 'prefix' and 'keep_vars'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_to_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: Module._save_to_state_dict() missing 2 required positional arguments: 'prefix' and 'keep_vars'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:12:40.668376Z",
     "start_time": "2024-08-15T08:12:40.668211Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('model.pth'))",
   "id": "f197bdd40d7e252d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
