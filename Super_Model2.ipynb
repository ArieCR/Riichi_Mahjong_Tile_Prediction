{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.154472Z",
     "start_time": "2024-08-14T20:48:05.513976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import sqlite3\n",
    "import torch\n",
    "import time\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('tenhou_dataprocess/dst/2016-2020_after_script_waits.db')\n",
    "cursor = conn.cursor()\n",
    "# Query the data\n",
    "start_time = time.time()\n",
    "cursor.execute('SELECT X_values, y_values FROM test_table')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process the data\n",
    "X_extracted = []\n",
    "y_extracted = []\n",
    "counter = 0\n",
    "for row in rows:\n",
    "    counter += 1\n",
    "    X_row = ast.literal_eval(row[0])  # Convert string back to list\n",
    "    y_row = ast.literal_eval(row[1])  # Convert string back to list\n",
    "    X_extracted.append(X_row)\n",
    "    y_extracted.append(y_row)\n",
    "    if(counter//1000 == 500):\n",
    "        break\n",
    "    if(counter % 1000 == 0):\n",
    "        print(f\"Processed {counter} rows\", end='\\r')\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "# Convert lists back to NumPy arrays or tensors if needed\n",
    "import numpy as np\n",
    "\n",
    "X_extracted = np.array(X_extracted)\n",
    "y_extracted = np.array(y_extracted)\n",
    "\n",
    "# Optionally, convert to tensors\n",
    "X = torch.tensor(X_extracted, dtype=torch.float)\n",
    "y = torch.tensor(y_extracted, dtype=torch.float)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(X.shape, y.shape)  # Check shapes of tensors"
   ],
   "id": "161e877252d31bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500000, 368]) torch.Size([500000, 34])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T20:57:43.159537Z",
     "start_time": "2024-08-14T20:57:43.155921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split the data into train, validation and test\n",
    "index1 = int(len(X)*0.8)\n",
    "index2 = int(len(X)*0.9)\n",
    "X_train = X[:index1]\n",
    "X_val = X[index1:index2]\n",
    "X_test = X[index2:]\n",
    "y_train = y[:index1]\n",
    "y_val = y[index1:index2]\n",
    "y_test = y[index2:]"
   ],
   "id": "f8f682db447d6a64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T10:20:12.582222Z",
     "start_time": "2024-08-15T09:57:04.601912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import  BertModel\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MahjongModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MahjongModel, self).__init__()\n",
    "        #self.embedding = nn.Embedding(37, 128)\n",
    "        #self.projection = nn.Linear(368, 400)\n",
    "        self.projection = nn.Linear(368, 768)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc2 = nn.Linear(768, 34)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        # Convert categorical features to embeddings\n",
    "        #x_emb = self.embedding(x_batch)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Project concatenated features to BERT's hidden size\n",
    "        #x = self.fc1(torch.cat((x_batch, x_emb)))  # Shape: (batch_size, hidden_size)\n",
    "        #x = self.fc1(x_batch)\n",
    "        # Add sequence length dimension for BERT\n",
    "        #x = x.unsqueeze(1)  # Shape: (batch_size, sequence_length=1, hidden_size)\n",
    "        x_batch = self.projection(x_batch)\n",
    "        \n",
    "        x_batch = x_batch.unsqueeze(1)\n",
    "        # Use BERT to process the combined features\n",
    "        outputs = self.bert(inputs_embeds=x_batch)[0]\n",
    "        \n",
    "        # Pass through classification head\n",
    "        outputs = self.fc2(outputs)\n",
    "        \n",
    "        y_hat = torch.sigmoid(outputs)\n",
    "        return y_hat\n",
    "    \n",
    "from torch.utils.data import Dataset \n",
    "class MahjongDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    \n",
    "def getindices(tensor):\n",
    "    mylist = []\n",
    "    for i in range(len(tensor)):\n",
    "        if tensor[i]:\n",
    "            mylist.append(i)\n",
    "    return mylist\n",
    "    \n",
    "    \n",
    "\n",
    "def eval(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        outputs = (outputs >= 0.5).float()\n",
    "        ran = random.randint(0, 1070)\n",
    "        print(getindices(outputs[ran][0]),getindices(y_val[ran]))\n",
    "        count = 0\n",
    "        for i in range(y_val.size(0)):\n",
    "            count += getindices(outputs[i][0]) == getindices(y_val[i])\n",
    "        total_samples = y_val.size(0)\n",
    "        misclassification_rate = 1.0 - (count / total_samples)\n",
    "        return misclassification_rate\n",
    "\n",
    "# Example Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MahjongModel().to(device)\n",
    "\n",
    "train_dataset = MahjongDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = Adam(model.parameters() ,lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    num_batches = 0\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "\n",
    "        \n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches)\n",
    "    print(\"Time taken for epoch \", epoch, \" is \", time.time()-start)\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    start = time.time()\n",
    "    print(\"miss is \", miss, \"Time taken for eval is \", time.time()-start)\n",
    "    "
   ],
   "id": "d7b85b05253696c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/tmp/ipykernel_5162/3654550642.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3654550642.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.2259479618400259\n",
      "[] [3, 22]\n",
      "miss is  1.0\n",
      "Epoch  1  train loss is:  0.1978073485405244\n",
      "[] [19, 22]\n",
      "miss is  1.0\n",
      "Epoch  2  train loss is:  0.19754531432676803\n",
      "[] [14]\n",
      "miss is  1.0\n",
      "Epoch  3  train loss is:  0.19738119791078446\n",
      "[] [6, 28]\n",
      "miss is  1.0\n",
      "Epoch  4  train loss is:  0.19725125044812936\n",
      "[] [10]\n",
      "miss is  1.0\n",
      "Epoch  5  train loss is:  0.1971239298391525\n",
      "[] [18, 21, 24]\n",
      "miss is  1.0\n",
      "Epoch  6  train loss is:  0.19699102344796482\n",
      "[] [10, 27]\n",
      "miss is  1.0\n",
      "Epoch  7  train loss is:  0.196775687701257\n",
      "[] [19, 22]\n",
      "miss is  1.0\n",
      "Epoch  8  train loss is:  0.19650366382144602\n",
      "[] [2, 5, 8]\n",
      "miss is  1.0\n",
      "Epoch  9  train loss is:  0.19627205997019473\n",
      "[] [2, 5]\n",
      "miss is  1.0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T21:38:08.222358Z",
     "start_time": "2024-08-14T21:38:08.220165Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bcef878cb297e6a6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-15T10:20:12.669856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Real training\n",
    "optimizer = Adam(model.parameters() ,lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "  \n",
    "for epoch in range(100): # can train for as long as you want\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bert.encoder.layer[-5:].parameters():\n",
    "        param.requires_grad = True\n",
    "    num_batches = 0\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_tf = torch.squeeze(outputs).to(device)\n",
    "        loss = loss_function(y_pred_tf, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch \", epoch, \" train loss is: \", epoch_loss/num_batches, \"Time taken for epoch \", epoch, \" is \", time.time()-start)\n",
    "    start = time.time()\n",
    "    miss = eval(model, X_val.to(device), y_val.to(device))\n",
    "    print(\"Miss is \", miss, \"Time taken for eval is \", time.time()-start)  \n"
   ],
   "id": "5d1d94de8fcb474c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5162/3654550642.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input': torch.tensor(self.data[idx], dtype=torch.float),\n",
      "/tmp/ipykernel_5162/3654550642.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.labels[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  train loss is:  0.19576499184302967 Time taken for epoch  0  is  107.45856070518494\n",
      "[] [19]\n",
      "Miss is  1.0 Time taken for eval is  40.15659713745117\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:12:40.667060Z",
     "start_time": "2024-08-15T08:12:40.650150Z"
    }
   },
   "cell_type": "code",
   "source": "model._save_to_state_dict('model.pth')",
   "id": "ee5556e7b80b08c0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module._save_to_state_dict() missing 2 required positional arguments: 'prefix' and 'keep_vars'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_to_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: Module._save_to_state_dict() missing 2 required positional arguments: 'prefix' and 'keep_vars'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:12:40.668376Z",
     "start_time": "2024-08-15T08:12:40.668211Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('model.pth'))",
   "id": "f197bdd40d7e252d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
